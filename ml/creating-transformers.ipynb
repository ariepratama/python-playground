{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References \n",
    "* https://www.datacamp.com/tutorial/building-a-transformer-with-py-torch\n",
    "\n",
    "# Motivations\n",
    "* Transformers is one of the building block for the modern LLMs.\n",
    "* It has been established far back from 2017\n",
    "* Understanding the foundation of LLM, might uncover how the module works and how it stores data and generelized it's weights so that it becames the current LLM.\n",
    "* Then possibly unlocking new discoveries on new neural networks techniques.\n",
    "\n",
    "# Components of transformers\n",
    "## Multi attention layer\n",
    "* computes attention between each pair of positions in a sequence.\n",
    "* It has multiple attention heads, capturing different aspect of input sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import math\n",
    "from tqdm import tqdm_notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dim, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert dim % num_heads == 0\n",
    "\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.dim_k = self.dim // self.num_heads  # dimensions for each head's key, query, and value\n",
    "        self.lin_query = nn.Linear(self.dim, self.dim)\n",
    "        self.lin_key = nn.Linear(self.dim, self.dim)\n",
    "        self.lin_val = nn.Linear(self.dim, self.dim)\n",
    "        self.lin_out = nn.Linear(self.dim, self.dim)\n",
    "    \n",
    "    def split_heads(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "            Split input, x, into multiple heads input\n",
    "                x           Tensor (batch_size, seq_length, dim)\n",
    "                return      Tensor (batch_size, num_heads, seq_length, dim_per_head)\n",
    "                            where \n",
    "                                dim_per_head = dim // num_heads\n",
    "            \"head\" might be the brain to pay attention and judge / specialized to certain number of \"segments\" of dimension in a sequence\n",
    "        \"\"\"\n",
    "        batch_size, seq_length, dim = x.size()\n",
    "        # say the x is in size (batch_size=1, seq_length=4, dim=4)\n",
    "        # torch.tensor([\n",
    "        #     [\n",
    "        #         [1,1,1,1], \n",
    "        #         [2,2,2,2],\n",
    "        #         [3,3,3,3],\n",
    "        #         [4,4,4,4]\n",
    "        #     ],\n",
    "        # ])\n",
    "        # then after view, it will split the dimension into (1, 4, 2, 2)\n",
    "        # tensor([\n",
    "        # [[[1, 1],\n",
    "        #   [1, 1]],\n",
    "\n",
    "        #  [[2, 2],\n",
    "        #   [2, 2]],\n",
    "\n",
    "        #  [[3, 3],\n",
    "        #   [3, 3]],\n",
    "\n",
    "        #  [[4, 4],\n",
    "        #   [4, 4]]]])\n",
    "        # then transpose will do (1, 2, 4, 2)\n",
    "        # tensor([\n",
    "        # [[[1, 1],\n",
    "        #   [2, 2],\n",
    "        #   [3, 3],\n",
    "        #   [4, 4]],\n",
    "\n",
    "        #  [[1, 1],\n",
    "        #   [2, 2],\n",
    "        #   [3, 3],\n",
    "        #   [4, 4]]]])\n",
    "        \n",
    "        return x.view(\n",
    "            batch_size, seq_length, self.num_heads, self.dim_k\n",
    "        ).transpose(1, 2)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor, mask=None):\n",
    "        \"\"\"\n",
    "            Q, K, V are Tensors with size (batch_size, n_heads, seq_length, dim_per_head)\n",
    "        \"\"\"\n",
    "        # calculate attention scores\n",
    "        # https://stackoverflow.com/questions/73924697/whats-the-difference-between-torch-mm-torch-matmul-and-torch-mul\n",
    "        # https://pytorch.org/docs/main/generated/torch.matmul.html\n",
    "        # broadcasting https://numpy.org/doc/stable/user/basics.broadcasting.html\n",
    "        # this will do matrix multiplication between (seq_length, dim_per_head) x (dim_per_head, seq_length) for each n_heads in all batch\n",
    "        # attn_scores = (batch_size, n_heads, dim_per_head, seq_length)\n",
    "        # then attn_scores will be // by sqrt(dim) (not sure why sqrt is chosen for scaling, why not other scaling factors?)\n",
    "        attn_scores = torch.matmul(\n",
    "            Q,\n",
    "            K.transpose(-2, -1)\n",
    "        ) / math.sqrt(self.dim_k)\n",
    "\n",
    "        # determine which attn_scores should be hidden or ignored with mask, for example:\n",
    "        # torch.tensor([\n",
    "        #     [0,1,1,1,0,1,1]\n",
    "        # ]).masked_fill(\n",
    "        #     torch.tensor([\n",
    "        #         [True,True,False,True,True,True,True]\n",
    "        #     ]), \n",
    "        #     33\n",
    "        # )\n",
    "        # tensor([[33, 33,  1, 33, 33, 33, 33]])\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "        # attn_scores = (batch_size, n_heads, dim_per_head, seq_length)\n",
    "        # V = (batch_size, n_heads, seq_length, dim_per_head)\n",
    "        # out = (batch_size, n_heads, seq_length, dim_per_head) -> following the second argument\n",
    "        out = torch.matmul(attn_probs, V)\n",
    "        return out\n",
    "\n",
    "    def combine_heads(self, multi_head_attns: torch.Tensor):\n",
    "        batch_size, _, seq_length, dim_k = multi_head_attns.size()\n",
    "        \n",
    "        # little notes about pytorch operation\n",
    "        # transpose = switch the dimensions of the tensor\n",
    "        # contiguous = make a new copy of the tensor and save it in new memory\n",
    "        return multi_head_attns.transpose(1, 2).contiguous().view(batch_size, seq_length, self.dim)\n",
    "    \n",
    "    def forward(self, Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor, mask=None):\n",
    "        \"\"\"\n",
    "         Q, K, V are Tensor with (batch_size, seq_length, dim)\n",
    "        \"\"\"\n",
    "        # split will turn each tensor into multiple heads tensors\n",
    "        # (batch_size, seq_length, dim) -> (batch_size, n_heads, seq_length, dim_per_head)\n",
    "        # each heads will get the same sequence length, but will only train for a specific set of dimensions\n",
    "        query_heads = self.split_heads(self.lin_query(Q))\n",
    "        key_heads = self.split_heads(self.lin_key(K))\n",
    "        val_heads = self.split_heads(self.lin_val(V))\n",
    "\n",
    "        attn_output = self.scaled_dot_product_attention(query_heads, key_heads, val_heads, mask)\n",
    "\n",
    "        out = self.lin_out(self.combine_heads(attn_output))\n",
    "        return out\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, dim: int, dim_ff: int):\n",
    "        \"\"\"\n",
    "            dim: hidden dimension\n",
    "            dim_ff: dimension of feed forward\n",
    "        \"\"\"\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.lin_1 = nn.Linear(dim, dim_ff)\n",
    "        self.lin_2 = nn.Linear(dim_ff, dim)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        y = self.lin_1(x)\n",
    "        y = self.relu(y)\n",
    "        y = self.lin_2(y)\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, dim: int, max_seq_length: int):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_seq_length, dim)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, dim, 2).float() * -(math.log(10000.0) / dim)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return x + self.pe[:, :x.size(1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, dim, n_heads, dim_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(dim, n_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(dim, dim_ff)\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor, return_attn=False):\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm1(x + self.dropout(ff_output))\n",
    "\n",
    "        if return_attn:\n",
    "            return x, attn_ouput\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, dim: int, n_heads: int, dim_ff: int, dropout: float):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(dim, n_heads)\n",
    "        self.cross_attn = MultiHeadAttention(dim, n_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(dim, dim_ff)\n",
    "        self.lin = nn.Linear(dim_ff, dim_ff)\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.norm3 = nn.LayerNorm(dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, enc_output: torch.Tensor, src_mask: torch.Tensor, tgt_mask: torch.Tensor):\n",
    "        attn_output = self.self_attn(x, x, x, mask=tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        attn_output = self.cross_attn(x, enc_output, enc_output, mask=src_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, dim, n_heads, n_layers, dim_ff, max_seq_length, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder_embedding = nn.Embedding(src_vocab_size, dim)\n",
    "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, dim)\n",
    "        self.positional_encoding = PositionalEncoding(dim, max_seq_length)\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            EncoderLayer(dim, n_heads, dim_ff, dropout) for _ in range(n_layers)\n",
    "        ])\n",
    "        self.decoder_layers = nn.ModuleList([\n",
    "            DecoderLayer(dim, n_heads, dim_ff, dropout) for _ in range(n_layers)\n",
    "        ])\n",
    "        self.fc = nn.Linear(dim, tgt_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def generate_mask(self, src, tgt):\n",
    "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
    "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
    "        seq_length = tgt.size(1)\n",
    "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n",
    "        tgt_mask = tgt_mask & nopeak_mask\n",
    "        return src_mask, tgt_mask\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
    "        src_embedded = self.dropout(\n",
    "            self.positional_encoding(\n",
    "                self.encoder_embedding(src)\n",
    "            )\n",
    "        )\n",
    "        tgt_embedded = self.dropout(\n",
    "            self.positional_encoding(\n",
    "                self.decoder_embedding(tgt)\n",
    "            )\n",
    "        )\n",
    "        enc_output = src_embedded\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_output = enc_layer(enc_output, src_mask)\n",
    "        \n",
    "        dec_output = tgt_embedded\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
    "\n",
    "        output = self.fc(dec_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"spdenisov/word_aligned_translation\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, start_idx=100, unk_token=\"[UNK]\", pad_token=\"[PAD]\", pad_token_id=10, max_seq_length=100):\n",
    "        self.start_idx = start_idx\n",
    "        self.unk_token = unk_token\n",
    "        self.pad_token = pad_token\n",
    "        self.pad_token_id = pad_token_id\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.word_to_id = {self.unk_token: start_idx}\n",
    "        self.id_to_word = {k: self.unk_token for k in range(start_idx)}\n",
    "        self.word_to_id[self.pad_token] = pad_token_id\n",
    "        self.id_to_word[self.pad_token_id] = self.pad_token\n",
    "\n",
    "    def train_tokenize(self, sentence):\n",
    "        splitted = sentence.split(\" \")\n",
    "        res = []\n",
    "        for s in splitted:\n",
    "            if s not in self.word_to_id:\n",
    "                max_id = max(self.word_to_id.values()) \n",
    "                cur_id = max_id + 1\n",
    "                self.word_to_id[s] = cur_id\n",
    "                self.id_to_word[cur_id] = s\n",
    "            \n",
    "            res.append(self.word_to_id[s]) \n",
    "        return res\n",
    "\n",
    "    def tokenize(self, sentence):\n",
    "        splitted = sentence.split(\" \")\n",
    "        res = []\n",
    "        for s in splitted:\n",
    "            if s not in self.word_to_id:\n",
    "                res.append(self.word_to_id[self.unk_token])    \n",
    "                continue\n",
    "            res.append(self.word_to_id[s]) \n",
    "        if len(res) < self.max_seq_length:\n",
    "            diff = self.max_seq_length - len(res)\n",
    "            for _ in range(diff):\n",
    "                res.append(self.word_to_id[self.pad_token])\n",
    "        if len(res) > self.max_seq_length:\n",
    "            res = res[:self.max_seq_length]\n",
    "        return res\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_tokenizer = Tokenizer()\n",
    "fr_tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_to_fr = ds.filter(\n",
    "    lambda x: x[\"target_language\"] == \"French\" and x[\"source_language\"] == \"English\")\n",
    "en_to_fr = en_to_fr.remove_columns([\"target_language\", \"source_language\"])\n",
    "en_to_fr = en_to_fr.map(\n",
    "    lambda x: {\n",
    "        \"source_sentence\": \" \".join([\"_\".join(w.split(\" \")[1:]) for w in x[\"source_words\"]]),\n",
    "        \"target_sentence\": \" \".join([\"_\".join(w.split(\" \")[1:]) for w in x[\"target_lines\"]])\n",
    "    }).remove_columns([\"source_words\", \"target_lines\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in en_to_fr:\n",
    "    en_tokenizer.train_tokenize(x[\"source_sentence\"])\n",
    "    fr_tokenizer.train_tokenize(x[\"target_sentence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fac11a7d0f54f5fbab0a5a0f96cf13e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/321 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "en_to_fr = en_to_fr.map(\n",
    "    lambda x: {\n",
    "        \"tokenized_source\": en_tokenizer.tokenize(x[\"source_sentence\"]),\n",
    "        \"tokenized_target\": fr_tokenizer.tokenize(x[\"target_sentence\"])\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitted = en_to_fr.train_test_split(test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitted[\"train\"].set_format(type=\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['source_sentence', 'target_sentence', 'tokenized_source', 'tokenized_target'],\n",
       "    num_rows: 224\n",
       "})"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitted[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = splitted[\"train\"].remove_columns([\"source_sentence\", \"target_sentence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_data,\n",
    "    batch_size=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = dict(\n",
    "    src_vocab_size = max(en_tokenizer.word_to_id.values()) + 100,\n",
    "    tgt_vocab_size = max(fr_tokenizer.word_to_id.values()) + 100,\n",
    "    dim = 256,\n",
    "    n_heads = 32,\n",
    "    n_layers = 1,\n",
    "    dim_ff = 256,\n",
    "    max_seq_length = 100,\n",
    "    dropout = 0.3\n",
    ")\n",
    "\n",
    "t = Transformer(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jt/nkdkjf5s7x13_glj3xkf5ky80000gn/T/ipykernel_52006/3476815096.py:7: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  pbar = tqdm_notebook(range(100))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7222d1f89044d0ebb441e58ff1723f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(t.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "t.train()\n",
    "\n",
    "epoch_losses = []\n",
    "pbar = tqdm_notebook(range(100))\n",
    "for epoch in pbar:\n",
    "    for batch in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        src_data = batch[\"tokenized_source\"]\n",
    "        tgt_data = batch[\"tokenized_target\"]\n",
    "        output = t(src_data, tgt_data[:, :-1])\n",
    "        loss = criterion(output.contiguous().view(-1, params[\"tgt_vocab_size\"]), tgt_data[:, 1:].contiguous().view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_losses.append(loss.item())\n",
    "        pbar.set_postfix({\"loss\": epoch_losses[-1]})\n",
    "        # print(f\"Epoch: {epoch+1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jt/nkdkjf5s7x13_glj3xkf5ky80000gn/T/ipykernel_52006/2284191234.py:1: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  pbar = tqdm_notebook(range(100))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc45a41aaf9347f693a5913809a0ad88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[149], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m output \u001b[38;5;241m=\u001b[39m t(src_data, tgt_data[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m      8\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtgt_vocab_size\u001b[39m\u001b[38;5;124m\"\u001b[39m]), tgt_data[:, \u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m----> 9\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     10\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     11\u001b[0m epoch_losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/da311/lib/python3.11/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    628\u001b[0m )\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/da311/lib/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m _engine_run_backward(\n\u001b[1;32m    348\u001b[0m     tensors,\n\u001b[1;32m    349\u001b[0m     grad_tensors_,\n\u001b[1;32m    350\u001b[0m     retain_graph,\n\u001b[1;32m    351\u001b[0m     create_graph,\n\u001b[1;32m    352\u001b[0m     inputs,\n\u001b[1;32m    353\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    354\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    355\u001b[0m )\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/da311/lib/python3.11/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    824\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    825\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pbar = tqdm_notebook(range(100))\n",
    "for epoch in pbar:\n",
    "    for batch in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        src_data = batch[\"tokenized_source\"]\n",
    "        tgt_data = batch[\"tokenized_target\"]\n",
    "        output = t(src_data, tgt_data[:, :-1])\n",
    "        loss = criterion(output.contiguous().view(-1, params[\"tgt_vocab_size\"]), tgt_data[:, 1:].contiguous().view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_losses.append(loss.item())\n",
    "        pbar.set_postfix({\"loss\": epoch_losses[-1]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(tokenizer, ten):\n",
    "    res = torch.argmax(ten, dim=-1).squeeze(0).detach().cpu().numpy()\n",
    "    result = []\n",
    "    for a in res:\n",
    "        if a in tokenizer.id_to_word:\n",
    "            result.append(tokenizer.id_to_word[a])\n",
    "            continue\n",
    "        result.append(tokenizer.id_to_word[tokenizer.unk_token])\n",
    "    return \" \".join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "volonté volonté volonté volonté volonté et et et , , , , et . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] . . . . . .\n"
     ]
    }
   ],
   "source": [
    "t.eval()\n",
    "\n",
    "# Generate random sample validation data\n",
    "# val_src_data = torch.randint(1, src_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)\n",
    "# val_tgt_data = torch.randint(1, tgt_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)\n",
    "\n",
    "with torch.no_grad():\n",
    "    sentence = \"You can filter fields in the row column , and detail areas .\"\n",
    "    tokenized = en_tokenizer.tokenize(sentence)\n",
    "    tokenized_tensor = torch.tensor([tokenized])\n",
    "    target_tensor = torch.ones(tokenized_tensor.size(), dtype=torch.long) * 12\n",
    "    \n",
    "    out = t(tokenized_tensor, target_tensor)\n",
    "    print(decode(fr_tokenizer, out))\n",
    "    # val_output = t(val_src_data, val_tgt_data[:, :-1])\n",
    "    # val_loss = criterion(val_output.contiguous().view(-1, tgt_vocab_size), val_tgt_data[:, 1:].contiguous().view(-1))\n",
    "    # print(f\"Validation Loss: {val_loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source_sentence': 'You can filter fields in the row , column , and detail areas .',\n",
       " 'target_sentence': 'Vous peut filtre les_champs dans le/la/les rangée , colonnes , et détail zones .',\n",
       " 'tokenized_source': [101,\n",
       "  102,\n",
       "  103,\n",
       "  104,\n",
       "  105,\n",
       "  106,\n",
       "  107,\n",
       "  108,\n",
       "  109,\n",
       "  108,\n",
       "  110,\n",
       "  111,\n",
       "  112,\n",
       "  113,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10],\n",
       " 'tokenized_target': [101,\n",
       "  102,\n",
       "  103,\n",
       "  104,\n",
       "  105,\n",
       "  106,\n",
       "  107,\n",
       "  108,\n",
       "  109,\n",
       "  108,\n",
       "  110,\n",
       "  111,\n",
       "  112,\n",
       "  113,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10]}"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_to_fr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "da311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
