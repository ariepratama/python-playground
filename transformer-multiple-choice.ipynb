{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76709272-e3f2-400e-925b-e363b07985f2",
   "metadata": {},
   "source": [
    "this example is taken from https://huggingface.co/docs/transformers/tasks/multiple_choice, with more explanation on the code itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8feb9ce2-53f1-4944-8fb8-b4680bf4cd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84d5fb25-b707-4980-9ba8-af3cea30d3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take only 1000 sample from train\n",
    "swag = load_dataset(\"swag\", split=\"train[:1000]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91914c94-e2a1-4176-90ab-c815bade3201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'video-id': 'anetv_jkn6uvmqwh4',\n",
       " 'fold-ind': '3416',\n",
       " 'startphrase': 'Members of the procession walk down the street holding small horn brass instruments. A drum line',\n",
       " 'sent1': 'Members of the procession walk down the street holding small horn brass instruments.',\n",
       " 'sent2': 'A drum line',\n",
       " 'gold-source': 'gold',\n",
       " 'ending0': 'passes by walking down the street playing their instruments.',\n",
       " 'ending1': 'has heard approaching them.',\n",
       " 'ending2': \"arrives and they're outside dancing and asleep.\",\n",
       " 'ending3': 'turns the lead singer watches the performance.',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swag[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24cd7509-ae73-41f0-9d5c-ee9b0eeb9deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2d29ba0-3cfb-405a-9df2-d29716655e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa69ee69-f9db-4739-9693-2a59e159f523",
   "metadata": {},
   "outputs": [],
   "source": [
    "ending_names = [\n",
    "    f\"ending{x}\"\n",
    "    for x in range(4)\n",
    "]\n",
    "\n",
    "def preprocess(batch):\n",
    "\n",
    "    # sent1 sample:\n",
    "    # sent1 = ['Members of the procession walk down the street holding small horn brass instruments.',\n",
    "    # 'A drum line passes by walking down the street playing their instruments.',\n",
    "    # 'A group of members in green uniforms walks waving flags.',\n",
    "    # 'A drum line passes by walking down the street playing their instruments.']\n",
    "    \n",
    "    # sent2 = ['A drum line',\n",
    "    # 'Members of the procession',\n",
    "    # 'Members of the procession',\n",
    "    # 'Members of the procession']\n",
    "            \n",
    "    # repeat the context as many as the choices, in this case 4.\n",
    "     # first_sentences =   ['Members of the procession walk down the street holding small horn brass instruments.',\n",
    "     # 'Members of the procession walk down the street holding small horn brass instruments.',\n",
    "     # 'Members of the procession walk down the street holding small horn brass instruments.',\n",
    "     # 'Members of the procession walk down the street holding small horn brass instruments.',\n",
    "    \n",
    "     # 'A drum line passes by walking down the street playing their instruments.',\n",
    "     # 'A drum line passes by walking down the street playing their instruments.',\n",
    "     # 'A drum line passes by walking down the street playing their instruments.',\n",
    "     # 'A drum line passes by walking down the street playing their instruments.',\n",
    "    \n",
    "     # 'A group of members in green uniforms walks waving flags.',\n",
    "     # 'A group of members in green uniforms walks waving flags.',\n",
    "     # 'A group of members in green uniforms walks waving flags.',\n",
    "     # 'A group of members in green uniforms walks waving flags.',\n",
    "    \n",
    "     # 'A drum line passes by walking down the street playing their instruments.',\n",
    "     # 'A drum line passes by walking down the street playing their instruments.',\n",
    "     # 'A drum line passes by walking down the street playing their instruments.',\n",
    "     # 'A drum line passes by walking down the street playing their instruments.']\n",
    "    first_sentences = [\n",
    "        [context] * 4\n",
    "        for context in batch[\"sent1\"]\n",
    "    ]\n",
    "    \n",
    "    question_headers = batch[\"sent2\"]\n",
    "    # add sent2 to each choice. e.g: sent2='A durm line'\n",
    "    # second_sentences = ['A drum line passes by walking down the street playing their instruments.',\n",
    "    #  'A drum line has heard approaching them.',\n",
    "    #  \"A drum line arrives and they're outside dancing and asleep.\",\n",
    "    #  'A drum line turns the lead singer watches the performance.',\n",
    "    #  'Members of the procession are playing ping pong and celebrating one left each in quick.',\n",
    "    #  'Members of the procession wait slowly towards the cadets.',\n",
    "    #  'Members of the procession continues to play as well along the crowd along with the band being interviewed.',\n",
    "    #  'Members of the procession continue to play marching, interspersed.',\n",
    "    #  'Members of the procession pay the other coaches to cheer as people this chatter dips in lawn sheets.',\n",
    "    #  'Members of the procession walk down the street holding small horn brass instruments.',\n",
    "    #  'Members of the procession is seen in the background.',\n",
    "    #  'Members of the procession are talking a couple of people playing a game of tug of war.',\n",
    "    #  'Members of the procession are playing ping pong and celebrating one left each in quick.',\n",
    "    #  'Members of the procession wait slowly towards the cadets.',\n",
    "    #  'Members of the procession makes a square call and ends by jumping down into snowy streets where fans begin to take their positions.',\n",
    "    #  'Members of the procession play and go back and forth hitting the drums while the audience claps for them.']\n",
    "    second_sentences = [\n",
    "        [f\"{header} {batch[end][i]}\" for end in ending_names]\n",
    "        for i, header in enumerate(question_headers)\n",
    "    ]\n",
    "    \n",
    "    # flattened the list [[s1, s2, s3, s4]] -> [s1, s2, s3, s4]\n",
    "    first_sentences = sum(first_sentences, [])\n",
    "    second_sentences = sum(second_sentences, [])\n",
    "\n",
    "    # we will need to append the context / the question into the beginning of the multiple choices.\n",
    "    # this will concatenate 2 sentences, from first + second, into 1 tokenized sentence\n",
    "    # sample: \n",
    "    #   sent1: ['Members of the procession walk down the street holding small horn brass instruments.']\n",
    "    #   sent2: ['A drum line passes by walking down the street playing their instruments.']\n",
    "    #   tokenized1: [101, 2372, 1997, 1996, 14385, 3328, 2091, 1996, 2395, 3173, 2235, 7109, 8782, 5693, 1012, 102] \n",
    "    #   tokenized2: [101, 1037, 6943, 2240, 5235, 2011, 3788, 2091, 1996, 2395, 2652, 2037, 5693, 1012, 102]\n",
    "    #   tokenized: [101, 2372, 1997, 1996, 14385, 3328, 2091, 1996, 2395, 3173, 2235, 7109, 8782, 5693, 1012, 102, \n",
    "    #                    1037, 6943, 2240, 5235, 2011, 3788, 2091, 1996, 2395, 2652, 2037, 5693, 1012, 102]\n",
    "    #   tokenized: Dict[str, array] = {\"input_ids\": [], \"attention_mask\": []}        \n",
    "    tokenized = tokenizer(\n",
    "        text=first_sentences, \n",
    "        text_pair=second_sentences, \n",
    "        truncation=True)\n",
    "    \n",
    "    res = {\n",
    "        k: [\n",
    "            v[i:i + 4]\n",
    "            for i in range(0, len(v), 4)\n",
    "           ]\n",
    "        for k, v in tokenized.items()\n",
    "    }\n",
    "    \n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c975616-bb36-4815-a1d8-5384d084fdc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "757dc898eb6041d8a37c95d9e4b4abf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_swag = swag.map(preprocess, batched=True, batch_size=4, remove_columns=[\n",
    "    \"video-id\",\n",
    "    \"startphrase\",\n",
    "    \"sent1\",\n",
    "    \"sent2\",\n",
    "    \"gold-source\",\n",
    "    \"ending0\",\n",
    "    \"ending1\",\n",
    "    \"ending2\",\n",
    "    \"ending3\",\n",
    "    \"fold-ind\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "164d21dd-e56f-40e8-b5cf-1465952d7156",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n",
    "from typing import Optional, Union\n",
    "import torch\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForMultipleChoice:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs for multiple choice received.\n",
    "    should be used in batch mode.\n",
    "    \"\"\"\n",
    "\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    padding: Union[bool, str, PaddingStrategy] = True\n",
    "    max_length: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features):\n",
    "        # features: [\n",
    "        #   {\"input_ids\": [[], [], [], []], \"attention_mask\": [[], [], [], []], \"labels\": [v,w,x,y]},\n",
    "        #   {\"input_ids\": [[], [], [], []], \"attention_mask\": [[], [], [], []], \"labels\": [v,w,x,y]},\n",
    "        # ]\n",
    "        label_name = \"label\" if \"label\" in features[0].keys() else \"labels\"\n",
    "\n",
    "        # remove the label from the features\n",
    "        labels = [feature.pop(label_name) for feature in features]\n",
    "        \n",
    "        batch_size = len(features)\n",
    "        num_choices = len(features[0][\"input_ids\"])\n",
    "        # assuming dataset batch_size is 4 and training batch_size is 4.\n",
    "        # features: [\n",
    "        #   {\"input_ids\": [[], [], [], []], \"attention_mask\": [[], [], [], []]},\n",
    "        #   {\"input_ids\": [[], [], [], []], \"attention_mask\": [[], [], [], []]},\n",
    "        # ]\n",
    "        # into----------\n",
    "        # flattened_features:[\n",
    "        #   [\n",
    "        #     {\"input_ids\": [], \"attention_mask\": []},\n",
    "        #     {\"input_ids\": [], \"attention_mask\": []},\n",
    "        #     {\"input_ids\": [], \"attention_mask\": []},\n",
    "        #     {\"input_ids\": [], \"attention_mask\": []},\n",
    "        #   ],\n",
    "        #   [\n",
    "        #     {\"input_ids\": [], \"attention_mask\": []},\n",
    "        #     {\"input_ids\": [], \"attention_mask\": []},\n",
    "        #     {\"input_ids\": [], \"attention_mask\": []},\n",
    "        #     {\"input_ids\": [], \"attention_mask\": []},\n",
    "        #   ],\n",
    "        # ]\n",
    "        #  then finally into\n",
    "        # flattened_features: [\n",
    "        #   {\"input_ids\": [], \"attention_mask\": []},\n",
    "        #   {\"input_ids\": [], \"attention_mask\": []},\n",
    "        #   {\"input_ids\": [], \"attention_mask\": []},\n",
    "        #   {\"input_ids\": [], \"attention_mask\": []},\n",
    "        #   {\"input_ids\": [], \"attention_mask\": []},\n",
    "        #   {\"input_ids\": [], \"attention_mask\": []},\n",
    "        #   {\"input_ids\": [], \"attention_mask\": []},\n",
    "        #   {\"input_ids\": [], \"attention_mask\": []},\n",
    "        # ]\n",
    "        flattened_features = [\n",
    "            [{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features\n",
    "        ]\n",
    "        flattened_features = sum(flattened_features, [])\n",
    "\n",
    "        # apply padding to flattened features\n",
    "        batch = self.tokenizer.pad(\n",
    "            flattened_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        \n",
    "        batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n",
    "        \n",
    "        # add back labels\n",
    "        batch[\"labels\"] = torch.tensor(labels, dtype=torch.int64)\n",
    "\n",
    "        # final batch form\n",
    "        # {input_ids: tensor(batch_size, num_choice, dim), attention_mask: tensor(batch_size, num_choice, dim), labels: tensor(batch_size)}\n",
    "        # example\n",
    "        # {'input_ids': tensor([\n",
    "        #   [\n",
    "        #     #batch 1\n",
    "        #     [  101, 12537,  4332,  2000,  3571,  2004,  2016,  4135,  2497,  4152,\n",
    "        #       3553,  1998,  3553,  1010,  2619,  2855,  4332,  1998, 19947,  1012,\n",
    "        #        102,  2619,  4332,  2006,  1996,  7525,  2598,  1998,  7288,  2000,\n",
    "        #       3338,  2091,  1012,   102,     0,     0,     0,     0,     0,     0],\n",
    "        #      ...\n",
    "        #   ]\n",
    "        #  ])\n",
    "        #  'attention_mask': tensor([\n",
    "        #   [\n",
    "        #    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "        #     1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
    "        #     ...\n",
    "        #   ],\n",
    "        #  ]),\n",
    "        # 'labels': tensor([3, 2, 2, 1])}\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "278bc9db-9b6b-4cd2-9ab1-30b3c596df6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = DataCollatorForMultipleChoice(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3e6f613b-1925-41dc-99ce-78baba1b3ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ff07654f-9d82-4621-8e4b-7c41f17e00a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6a23ad13-7275-4aa7-8f6e-39860f8a34ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForMultipleChoice were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForMultipleChoice, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForMultipleChoice.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a63850a6-57bb-46d5-aaed-ae80691b761c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='315' max='315' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [315/315 03:51, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.717541</td>\n",
       "      <td>0.826000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.190584</td>\n",
       "      <td>0.959000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.060478</td>\n",
       "      <td>0.986000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.028441</td>\n",
       "      <td>0.997000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.019747</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory my_awesome_swag_model/checkpoint-63 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=315, training_loss=0.36419539678664437, metrics={'train_runtime': 232.7147, 'train_samples_per_second': 21.486, 'train_steps_per_second': 1.354, 'total_flos': 282680387438592.0, 'train_loss': 0.36419539678664437, 'epoch': 5.0})"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"my_awesome_swag_model\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_swag,\n",
    "    eval_dataset=tokenized_swag,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForMultipleChoice(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6a571b5d-c614-47bf-8016-af59aee1b5bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['video-id', 'fold-ind', 'startphrase', 'sent1', 'sent2', 'gold-source', 'ending0', 'ending1', 'ending2', 'ending3', 'label', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_swag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ac0b8e-c8ea-4e8f-bd59-4007e3ca9a75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
